 تمرین 9.2.1  این تمرین به بررسی **فاصله کسینوسی** (Cosine Distance) بین بردارهای مشخصه (Feature Vectors) کامپیوترها، با در نظر گرفتن فاکتورهای مقیاس‌بندی مختلف می‌پردازد. فاصله کسینوسی در فضاهایی که دارای ابعاد هستند، از جمله فضاهای اقلیدسی، کاربرد دارد و می‌تواند برای اندازه‌گیری شباهت بین اقلام یا کاربران در سیستم‌های توصیه‌گر استفاده شود.

**تعریف بردارهای مشخصه برای کامپیوترها:**
بر اساس مقادیر داده شده در جدول، بردارهای مشخصه برای هر کامپیوتر به شرح زیر است:
*   **کامپیوتر A**: [سرعت پردازنده: 3.06, حجم دیسک: 500, حجم حافظه اصلی: 6]
*   **کامپیوتر B**: [سرعت پردازنده: 2.68, حجم دیسک: 320, حجم حافظه اصلی: 4]
*   **کامپیوتر C**: [سرعت پردازنده: 2.92, حجم دیسک: 640, حجم حافظه اصلی: 6]

همانطور که در منبع اشاره شده است، اگر این اجزا مقیاس‌بندی نشوند، مولفه‌ی **حجم دیسک** به دلیل مقادیر بسیار بزرگتر خود نسبت به سایر مولفه‌ها، بر محاسبه‌ی شباهت غلبه خواهد کرد. بنابراین، فاکتورهای مقیاس‌بندی برای هر مولفه در نظر گرفته می‌شود:
*   سرعت پردازنده: 1
*   حجم دیسک: $\alpha$
*   حجم حافظه اصلی: $\beta$

بردارهای مقیاس‌بندی شده برای هر کامپیوتر:
*   **A_scaled** = $[3.06 \times 1, 500 \times \alpha, 6 \times \beta] = [3.06, 500\alpha, 6\beta]$
*   **B_scaled** = $[2.68 \times 1, 320 \times \alpha, 4 \times \beta] = [2.68, 320\alpha, 4\beta]$
*   **C_scaled** = $[2.92 \times 1, 640 \times \alpha, 6 \times \beta] = [2.92, 640\alpha, 6\beta]$

**فرمول‌های مورد نیاز:**
برای محاسبه **فاصله کسینوسی** بین دو بردار **x** و **y**، ابتدا **حاصل‌ضرب داخلی** (Dot Product) و **نُرم L2** (L2-norm) هر بردار را محاسبه می‌کنیم:

*   **حاصل‌ضرب داخلی:** $\mathbf{x} \cdot \mathbf{y} = \sum_{i=1}^{n} x_i y_i$
*   **نُرم L2:** $||\mathbf{x}|| = \sqrt{\sum_{i=1}^{n} x_i^2}$
*   **کسینوس زاویه بین دو بردار ($\theta$):** $\cos(\theta) = \frac{\mathbf{x} \cdot \mathbf{y}}{||\mathbf{x}|| \cdot ||\mathbf{y}||}$
*   **زاویه بر حسب درجه:** $\theta = \arccos(\cos(\theta)) \times \frac{180}{\pi}$

---

### **(a) محاسبه کسینوس زوایا بر حسب $\alpha$ و $\beta$:**

**1. برای جفت (A, B):**
*   **حاصل‌ضرب داخلی:**
    $\mathbf{A} \cdot \mathbf{B} = (3.06)(2.68) + (500\alpha)(320\alpha) + (6\beta)(4\beta)$
    $= 8.2008 + 160000\alpha^2 + 24\beta^2$
*   **نُرم A به توان 2:**
    $||\mathbf{A}||^2 = (3.06)^2 + (500\alpha)^2 + (6\beta)^2$
    $= 9.3636 + 250000\alpha^2 + 36\beta^2$
*   **نُرم B به توان 2:**
    $||\mathbf{B}||^2 = (2.68)^2 + (320\alpha)^2 + (4\beta)^2$
    $= 7.1824 + 102400\alpha^2 + 16\beta^2$
*   **کسینوس زاویه (A, B):**
    $\cos(\theta_{AB}) = \frac{8.2008 + 160000\alpha^2 + 24\beta^2}{\sqrt{(9.3636 + 250000\alpha^2 + 36\beta^2) \times (7.1824 + 102400\alpha^2 + 16\beta^2)}}$

**2. برای جفت (A, C):**
*   **حاصل‌ضرب داخلی:**
    $\mathbf{A} \cdot \mathbf{C} = (3.06)(2.92) + (500\alpha)(640\alpha) + (6\beta)(6\beta)$
    $= 8.9352 + 320000\alpha^2 + 36\beta^2$
*   **نُرم A به توان 2:** (همان بالا)
    $||\mathbf{A}||^2 = 9.3636 + 250000\alpha^2 + 36\beta^2$
*   **نُرم C به توان 2:**
    $||\mathbf{C}||^2 = (2.92)^2 + (640\alpha)^2 + (6\beta)^2$
    $= 8.5264 + 409600\alpha^2 + 36\beta^2$
*   **کسینوس زاویه (A, C):**
    $\cos(\theta_{AC}) = \frac{8.9352 + 320000\alpha^2 + 36\beta^2}{\sqrt{(9.3636 + 250000\alpha^2 + 36\beta^2) \times (8.5264 + 409600\alpha^2 + 36\beta^2)}}$

**3. برای جفت (B, C):**
*   **حاصل‌ضرب داخلی:**
    $\mathbf{B} \cdot \mathbf{C} = (2.68)(2.92) + (320\alpha)(640\alpha) + (4\beta)(6\beta)$
    $= 7.8256 + 204800\alpha^2 + 24\beta^2$
*   **نُرم B به توان 2:** (همان بالا)
    $||\mathbf{B}||^2 = 7.1824 + 102400\alpha^2 + 16\beta^2$
*   **نُرم C به توان 2:** (همان بالا)
    $||\mathbf{C}||^2 = 8.5264 + 409600\alpha^2 + 36\beta^2$
*   **کسینوس زاویه (B, C):**
    $\cos(\theta_{BC}) = \frac{7.8256 + 204800\alpha^2 + 24\beta^2}{\sqrt{(7.1824 + 102400\alpha^2 + 16\beta^2) \times (8.5264 + 409600\alpha^2 + 36\beta^2)}}$

---

### **(b) محاسبه زوایا اگر $\alpha = 1$ و $\beta = 1$:**

در این حالت، هیچ مقیاس‌بندی اعمال نمی‌شود و انتظار می‌رود که مولفه‌ی حجم دیسک (با مقادیر 500, 320, 640) بر محاسبات غلبه کند و منجر به زوایای بسیار کوچک (شباهت بالا) شود، حتی اگر در واقعیت کامپیوترها تفاوت‌های قابل توجهی داشته باشند.

**بردارهای مقیاس‌بندی شده:**
*   A_scaled = $[3.06, 500, 6]$
*   B_scaled = $[2.68, 320, 4]$
*   C_scaled = $[2.92, 640, 6]$

**1. برای جفت (A, B):**
*   $\mathbf{A} \cdot \mathbf{B} = 8.2008 + 160000 + 24 = 160032.2008$
*   $||\mathbf{A}|| = \sqrt{9.3636 + 250000 + 36} = \sqrt{250045.3636} \approx 500.045$
*   $||\mathbf{B}|| = \sqrt{7.1824 + 102400 + 16} = \sqrt{102423.1824} \approx 320.036$
*   $\cos(\theta_{AB}) = \frac{160032.2008}{500.045 \times 320.036} = \frac{160032.2008}{160033.911} \approx \mathbf{0.999989}$
*   $\theta_{AB} = \arccos(0.999989) \approx \mathbf{0.25 \text{ degrees}}$

**2. برای جفت (A, C):**
*   $\mathbf{A} \cdot \mathbf{C} = 8.9352 + 320000 + 36 = 320044.9352$
*   $||\mathbf{A}|| \approx 500.045$
*   $||\mathbf{C}|| = \sqrt{8.5264 + 409600 + 36} = \sqrt{409654.5264} \approx 640.042$
*   $\cos(\theta_{AC}) = \frac{320044.9352}{500.045 \times 640.042} = \frac{320044.9352}{320046.225} \approx \mathbf{0.9999959}$
*   $\theta_{AC} = \arccos(0.9999959) \approx \mathbf{0.17 \text{ degrees}}$

**3. برای جفت (B, C):**
*   $\mathbf{B} \cdot \mathbf{C} = 7.8256 + 204800 + 24 = 204831.8256$
*   $||\mathbf{B}|| \approx 320.036$
*   $||\mathbf{C}|| \approx 640.042$
*   $\cos(\theta_{BC}) = \frac{204831.8256}{320.036 \times 640.042} = \frac{204831.8256}{204832.88} \approx \mathbf{0.9999948}$
*   $\theta_{BC} = \arccos(0.9999948) \approx \mathbf{0.21 \text{ degrees}}$

**نتیجه‌گیری برای (b):**
همانطور که مشاهده می‌شود، بدون مقیاس‌بندی مناسب، تمام کسینوس‌های زاویه بسیار نزدیک به 1 و زوایا بسیار کوچک (نزدیک به 0 درجه) هستند. این نشان می‌دهد که تمامی کامپیوترها بسیار شبیه به هم به نظر می‌رسند، که به دلیل غلبه‌ی مولفه‌ی حجم دیسک در محاسبه‌ی نُرم و حاصل‌ضرب داخلی است.

---

### **(c) محاسبه زوایا اگر $\alpha = 0.01$ و $\beta = 0.5$:**

با اعمال این فاکتورهای مقیاس‌بندی، انتظار می‌رود که تأثیر مولفه‌های مختلف در محاسبه‌ی شباهت متعادل‌تر شود و تفاوت‌های واقعی بین کامپیوترها منعکس گردد.

**بردارهای مقیاس‌بندی شده:**
*   A_scaled = $[3.06, 500 \times 0.01, 6 \times 0.5] = [3.06, 5, 3]$
*   B_scaled = $[2.68, 320 \times 0.01, 4 \times 0.5] = [2.68, 3.2, 2]$
*   C_scaled = $[2.92, 640 \times 0.01, 6 \times 0.5] = [2.92, 6.4, 3]$

**1. برای جفت (A, B):**
*   $\mathbf{A} \cdot \mathbf{B} = (3.06)(2.68) + (5)(3.2) + (3)(2) = 8.2008 + 16 + 6 = 30.2008$
*   $||\mathbf{A}|| = \sqrt{(3.06)^2 + 5^2 + 3^2} = \sqrt{9.3636 + 25 + 9} = \sqrt{43.3636} \approx 6.585$
*   $||\mathbf{B}|| = \sqrt{(2.68)^2 + (3.2)^2 + 2^2} = \sqrt{7.1824 + 10.24 + 4} = \sqrt{21.4224} \approx 4.629$
*   $\cos(\theta_{AB}) = \frac{30.2008}{6.585 \times 4.629} = \frac{30.2008}{30.505} \approx \mathbf{0.990}$
*   $\theta_{AB} = \arccos(0.990) \approx \mathbf{8.11 \text{ degrees}}$

**2. برای جفت (A, C):**
*   $\mathbf{A} \cdot \mathbf{C} = (3.06)(2.92) + (5)(6.4) + (3)(3) = 8.9352 + 32 + 9 = 49.9352$
*   $||\mathbf{A}|| \approx 6.585$
*   $||\mathbf{C}|| = \sqrt{(2.92)^2 + (6.4)^2 + 3^2} = \sqrt{8.5264 + 40.96 + 9} = \sqrt{58.4864} \approx 7.648$
*   $\cos(\theta_{AC}) = \frac{49.9352}{6.585 \times 7.648} = \frac{49.9352}{50.407} \approx \mathbf{0.9906}$
*   $\theta_{AC} = \arccos(0.9906) \approx \mathbf{7.78 \text{ degrees}}$

**3. برای جفت (B, C):**
*   $\mathbf{B} \cdot \mathbf{C} = (2.68)(2.92) + (3.2)(6.4) + (2)(3) = 7.8256 + 20.48 + 6 = 34.3056$
*   $||\mathbf{B}|| \approx 4.629$
*   $||\mathbf{C}|| \approx 7.648$
*   $\cos(\theta_{BC}) = \frac{34.3056}{4.629 \times 7.648} = \frac{34.3056}{35.405} \approx \mathbf{0.9689}$
*   $\theta_{BC} = \arccos(0.9689) \approx \mathbf{14.39 \text{ degrees}}$

**نتیجه‌گیری برای (c):**
با اعمال فاکتورهای مقیاس‌بندی $\alpha = 0.01$ و $\beta = 0.5$، زوایا بزرگتر شده‌اند و تفاوت‌های بیشتری را بین جفت‌های کامپیوترها نشان می‌دهند. این امر منعکس‌کننده‌ی تأثیر متعادل‌تر هر مولفه‌ی مشخصه بر روی محاسبه‌ی شباهت است، به طوری که هیچ مولفه‌ای به تنهایی بر نتیجه غلبه نمی‌کند. این نشان می‌دهد که کامپیوترهای A و C (حدود 7.78 درجه) کمی شبیه‌تر از A و B (حدود 8.11 درجه) هستند، و B و C (حدود 14.39 درجه) کمترین شباهت را در بین این سه جفت دارند.



تمرین 9.3.1: شکل 9.8 یک ماتریس امتیازدهی است که نشان‌دهنده امتیازهای کاربران A، B و C به آیتم‌های a تا h بر اساس مقیاس 1-5 است. از داده‌های این ماتریس، مراحل زیر را انجام دهید:
(a) ماتریس امتیازدهی را به صورت بولی تبدیل کنید، سپس فاصله جاکارد بین هر جفت کاربر را محاسبه کنید.

(b) بخش (a) را تکرار کنید، اما این بار از فاصله کسینوسی استفاده کنید.

(c) امتیازهای 3، 4 و 5 را به عنوان 1 و امتیازهای 1 و 2 را به عنوان 0 در نظر بگیرید و فاصله جاکارد بین هر جفت کاربر را محاسبه کنید.

(d) بخش (c) را تکرار کنید، اما این بار از فاصله کسینوسی استفاده کنید.

(e) ماتریس نرمال‌شده را با کسر میانگین از هر مقدار غیرصفر به دست آورید.

(f) از ماتریس به‌دست‌آمده در بخش (e)، فاصله کسینوسی بین هر جفت کاربر را محاسبه کنید


| کاربر/آیتم | a | b | c | d | e | f | g | h |
|:----------|:--|:--|:--|:--|:--|:--|:--|:--|
| **A** | 4 | 5 | 0 | 5 | 1 | 0 | 3 | 2 |
| **B** | 0 | 3 | 4 | 3 | 1 | 2 | 1 | 0 |
| **C** | 2 | 0 | 1 | 3 | 0 | 4 | 5 | 3 |

نکته مهم این است که در ماتریس‌های سودمندی، "صفر" معمولاً به معنای رتبه‌بندی نشده (خالی) یا عدم ترجیح است. با توجه به متن تمرینات در منبع که به "blank" اشاره می‌کند، مقادیر صفر در ماتریس شما (مانند C=0 یا F=0 برای کاربر A) را برای قسمت‌های (الف) و (ب) به عنوان **خالی (Unrated)** در نظر می‌گیرم. اما برای قسمت‌های (ج) و (د) که قانون خاصی برای تبدیل رتبه‌ها به صفر و یک ارائه شده، طبق آن قانون عمل خواهم کرد.

---

### حل تمرین 9.3.1

**ماتریس سودمندی اولیه (با در نظر گرفتن 0 به عنوان خانه خالی):**

| کاربر/آیتم | a | b | c | d | e | f | g | h |
|:----------|:--|:--|:--|:--|:--|:--|:--|:--|
| **A** | 4 | 5 | - | 5 | 1 | - | 3 | 2 |
| **B** | - | 3 | 4 | 3 | 1 | 2 | 1 | - |
| **C** | 2 | - | 1 | 3 | - | 4 | 5 | 3 |

---

**(a) محاسبه فاصله ژاکارد بین هر جفت کاربر، با فرض ماتریس سودمندی به صورت بولی:**

ابتدا ماتریس را به صورت بولی (1 برای آیتم رتبه‌بندی شده، 0 برای آیتم رتبه‌بندی نشده) تبدیل می‌کنیم.

| کاربر/آیتم | a | b | c | d | e | f | g | h |
|:----------|:--|:--|:--|:--|:--|:--|:--|:--|
| **A** | 1 | 1 | 0 | 1 | 1 | 0 | 1 | 1 |
| **B** | 0 | 1 | 1 | 1 | 1 | 1 | 1 | 0 |
| **C** | 1 | 0 | 1 | 1 | 0 | 1 | 1 | 1 |

*   **کاربران A و B:**
    *   آیتم‌های رتبه‌بندی شده توسط A (مجموعه $C_A$): {a, b, d, e, g, h}
    *   آیتم‌های رتبه‌بندی شده توسط B (مجموعه $C_B$): {b, c, d, e, f, g}
    *   اشتراک $C_A \cap C_B$: {b, d, e, g} (اندازه = 4)
    *   اجتماع $C_A \cup C_B$: {a, b, c, d, e, f, g, h} (اندازه = 8)
    *   شباهت ژاکارد $sim(A,B) = |C_A \cap C_B| / |C_A \cup C_B| = 4/8 = 0.5$.
    *   **فاصله ژاکارد $d(A,B) = 1 - sim(A,B) = 1 - 0.5 = 0.5$**.

*   **کاربران A و C:**
    *   آیتم‌های رتبه‌بندی شده توسط A ($C_A$): {a, b, d, e, g, h}
    *   آیتم‌های رتبه‌بندی شده توسط C ($C_C$): {a, c, d, f, g, h}
    *   اشتراک $C_A \cap C_C$: {a, d, g, h} (اندازه = 4)
    *   اجتماع $C_A \cup C_C$: {a, b, c, d, e, f, g, h} (اندازه = 8)
    *   شباهت ژاکارد $sim(A,C) = 4/8 = 0.5$.
    *   **فاصله ژاکارد $d(A,C) = 1 - 0.5 = 0.5$**.

*   **کاربران B و C:**
    *   آیتم‌های رتبه‌بندی شده توسط B ($C_B$): {b, c, d, e, f, g}
    *   آیتم‌های رتبه‌بندی شده توسط C ($C_C$): {a, c, d, f, g, h}
    *   اشتراک $C_B \cap C_C$: {c, d, f, g} (اندازه = 4)
    *   اجتماع $C_B \cup C_C$: {a, b, c, d, e, f, g, h} (اندازه = 8)
    *   شباهت ژاکارد $sim(B,C) = 4/8 = 0.5$.
    *   **فاصله ژاکارد $d(B,C) = 1 - 0.5 = 0.5$**.

---

**(b) تکرار قسمت (الف) با استفاده از فاصله کسینوسی:**

برای محاسبه فاصله کسینوسی، بردارهای بولی کامل (با 0 برای آیتم‌های رتبه‌بندی نشده) استفاده می‌شوند. شباهت کسینوسی برابر است با حاصل ضرب داخلی دو بردار تقسیم بر حاصل ضرب اندازه (نرم) آن‌ها. فاصله کسینوسی برابر با $1 - \text{شباهت کسینوسی}$.

*   **کاربران A و B:**
    *   $V_A =$
    *   $V_B =$
    *   حاصل ضرب داخلی $V_A \cdot V_B = 4$.
    *   اندازه $||V_A|| = \sqrt{1^2+1^2+0^2+1^2+1^2+0^2+1^2+1^2} = \sqrt{6}$.
    *   اندازه $||V_B|| = \sqrt{0^2+1^2+1^2+1^2+1^2+1^2+1^2+0^2} = \sqrt{6}$.
    *   شباهت کسینوسی $sim(A,B) = \frac{4}{\sqrt{6} \cdot \sqrt{6}} = \frac{4}{6} \approx 0.6667$.
    *   **فاصله کسینوسی $d(A,B) = 1 - 0.6667 = 0.3333$**.

*   **کاربران A و C:**
    *   $V_A =$
    *   $V_C =$
    *   حاصل ضرب داخلی $V_A \cdot V_C = 4$.
    *   اندازه $||V_A|| = \sqrt{6}$.
    *   اندازه $||V_C|| = \sqrt{6}$.
    *   شباهت کسینوسی $sim(A,C) = \frac{4}{\sqrt{6} \cdot \sqrt{6}} = \frac{4}{6} \approx 0.6667$.
    *   **فاصله کسینوسی $d(A,C) = 1 - 0.6667 = 0.3333$**.

*   **کاربران B و C:**
    *   $V_B =$
    *   $V_C =$
    *   حاصل ضرب داخلی $V_B \cdot V_C = 4$.
    *   اندازه $||V_B|| = \sqrt{6}$.
    *   اندازه $||V_C|| = \sqrt{6}$.
    *   شباهت کسینوسی $sim(B,C) = \frac{4}{\sqrt{6} \cdot \sqrt{6}} = \frac{4}{6} \approx 0.6667$.
    *   **فاصله کسینوسی $d(B,C) = 1 - 0.6667 = 0.3333$**.

---

**(c) رتبه‌بندی‌های 3، 4، و 5 را 1 و 1، 2، و خانه‌های خالی را 0 در نظر بگیرید. فاصله ژاکارد را بین هر جفت کاربر محاسبه کنید.**

ماتریس بولی جدید (بر اساس مقدار رتبه‌بندی):

| کاربر/آیتم | a | b | c | d | e | f | g | h |
|:----------|:--|:--|:--|:--|:--|:--|:--|:--|
| **A** | 1 (4) | 1 (5) | 0 (0) | 1 (5) | 0 (1) | 0 (0) | 1 (3) | 0 (2) |
| **B** | 0 (0) | 1 (3) | 1 (4) | 1 (3) | 0 (1) | 0 (2) | 0 (1) | 0 (0) |
| **C** | 0 (2) | 0 (0) | 0 (1) | 1 (3) | 0 (0) | 1 (4) | 1 (5) | 1 (3) |

*   **کاربران A و B:**
    *   آیتم‌های دارای 1 برای A ($S_A$): {a, b, d, g}
    *   آیتم‌های دارای 1 برای B ($S_B$): {b, c, d}
    *   اشتراک $S_A \cap S_B$: {b, d} (اندازه = 2)
    *   اجتماع $S_A \cup S_B$: {a, b, c, d, g} (اندازه = 5)
    *   شباهت ژاکارد $sim(A,B) = 2/5 = 0.4$.
    *   **فاصله ژاکارد $d(A,B) = 1 - 0.4 = 0.6$**.

*   **کاربران A و C:**
    *   آیتم‌های دارای 1 برای A ($S_A$): {a, b, d, g}
    *   آیتم‌های دارای 1 برای C ($S_C$): {d, f, g, h}
    *   اشتراک $S_A \cap S_C$: {d, g} (اندازه = 2)
    *   اجتماع $S_A \cup S_C$: {a, b, d, f, g, h} (اندازه = 6)
    *   شباهت ژاکارد $sim(A,C) = 2/6 \approx 0.3333$.
    *   **فاصله ژاکارد $d(A,C) = 1 - 0.3333 = 0.6667$**.

*   **کاربران B و C:**
    *   آیتم‌های دارای 1 برای B ($S_B$): {b, c, d}
    *   آیتم‌های دارای 1 برای C ($S_C$): {d, f, g, h}
    *   اشتراک $S_B \cap S_C$: {d} (اندازه = 1)
    *   اجتماع $S_B \cup S_C$: {b, c, d, f, g, h} (اندازه = 6)
    *   شباهت ژاکارد $sim(B,C) = 1/6 \approx 0.1667$.
    *   **فاصله ژاکارد $d(B,C) = 1 - 0.1667 = 0.8333$**.

---

**(d) تکرار قسمت (ج) با استفاده از فاصله کسینوسی:**

بردارهای بولی جدید (با 0 به عنوان مولفه صریح):

| کاربر/آیتم | a | b | c | d | e | f | g | h |
|:----------|:--|:--|:--|:--|:--|:--|:--|:--|
| **A** | 1 | 1 | 0 | 1 | 0 | 0 | 1 | 0 |
| **B** | 0 | 1 | 1 | 1 | 0 | 0 | 0 | 0 |
| **C** | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 |

*   **کاربران A و B:**
    *   $V_A' =$
    *   $V_B' =$
    *   حاصل ضرب داخلی $V_A' \cdot V_B' = (1*0) + (1*1) + (0*1) + (1*1) + (0*0) + (0*0) + (1*0) + (0*0) = 2$.
    *   اندازه $||V_A'|| = \sqrt{1^2+1^2+0^2+1^2+0^2+0^2+1^2+0^2} = \sqrt{4} = 2$.
    *   اندازه $||V_B'|| = \sqrt{0^2+1^2+1^2+1^2+0^2+0^2+0^2+0^2} = \sqrt{3} \approx 1.732$.
    *   شباهت کسینوسی $sim(A,B) = \frac{2}{2 \cdot 1.732} = \frac{2}{3.464} \approx 0.5774$.
    *   **فاصله کسینوسی $d(A,B) = 1 - 0.5774 = 0.4226$**.

*   **کاربران A و C:**
    *   $V_A' =$
    *   $V_C' =$
    *   حاصل ضرب داخلی $V_A' \cdot V_C' = (1*0) + (1*0) + (0*0) + (1*1) + (0*0) + (0*1) + (1*1) + (0*1) = 2$.
    *   اندازه $||V_A'|| = 2$.
    *   اندازه $||V_C'|| = \sqrt{0^2+0^2+0^2+1^2+0^2+1^2+1^2+1^2} = \sqrt{4} = 2$.
    *   شباهت کسینوسی $sim(A,C) = \frac{2}{2 \cdot 2} = \frac{2}{4} = 0.5$.
    *   **فاصله کسینوسی $d(A,C) = 1 - 0.5 = 0.5$**.

*   **کاربران B و C:**
    *   $V_B' =$
    *   $V_C' =$
    *   حاصل ضرب داخلی $V_B' \cdot V_C' = (0*0) + (1*0) + (1*0) + (1*1) + (0*0) + (0*1) + (0*1) + (0*1) = 1$.
    *   اندازه $||V_B'|| = \sqrt{3}$.
    *   اندازه $||V_C'|| = 2$.
    *   شباهت کسینوسی $sim(B,C) = \frac{1}{\sqrt{3} \cdot 2} = \frac{1}{3.464} \approx 0.2887$.
    *   **فاصله کسینوسی $d(B,C) = 1 - 0.2887 = 0.7113$**.

---

**(e) نرمال‌سازی ماتریس با کسر میانگین امتیاز هر کاربر از هر ورودی غیرخالی آن کاربر:**

میانگین رتبه‌بندی‌های هر کاربر (فقط آیتم‌های رتبه‌بندی شده):
*   **کاربر A:** رتبه‌ها: {4, 5, 5, 1, 3, 2}. مجموع = 20. تعداد = 6. میانگین = $20/6 \approx 3.333$.
*   **کاربر B:** رتبه‌ها: {3, 4, 3, 1, 2, 1}. مجموع = 14. تعداد = 6. میانگین = $14/6 \approx 2.333$.
*   **کاربر C:** رتبه‌ها: {2, 1, 3, 4, 5, 3}. مجموع = 18. تعداد = 6. میانگین = $18/6 = 3.0$.

**ماتریس نرمال‌سازی شده (مقدار اصلی - میانگین کاربر):**

| کاربر/آیتم | a | b | c | d | e | f | g | h |
|:----------|:--|:--|:--|:--|:--|:--|:--|:--|
| **A** | $4-3.333=\mathbf{0.667}$ | $5-3.333=\mathbf{1.667}$ | - | $5-3.333=\mathbf{1.667}$ | $1-3.333=\mathbf{-2.333}$ | - | $3-3.333=\mathbf{-0.333}$ | $2-3.333=\mathbf{-1.333}$ |
| **B** | - | $3-2.333=\mathbf{0.667}$ | $4-2.333=\mathbf{1.667}$ | $3-2.333=\mathbf{0.667}$ | $1-2.333=\mathbf{-1.333}$ | $2-2.333=\mathbf{-0.333}$ | $1-2.333=\mathbf{-1.333}$ | - |
| **C** | $2-3.0=\mathbf{-1.0}$ | - | $1-3.0=\mathbf{-2.0}$ | $3-3.0=\mathbf{0.0}$ | - | $4-3.0=\mathbf{1.0}$ | $5-3.0=\mathbf{2.0}$ | $3-3.0=\mathbf{0.0}$ |

---

**(f) با استفاده از ماتریس نرمال‌شده از قسمت (ه)، فاصله کسینوسی را بین هر جفت کاربر محاسبه کنید.**

فقط آیتم‌هایی که توسط هر دو کاربر در یک جفت رتبه‌بندی شده‌اند (غیر خالی هستند) در نظر گرفته می‌شوند.

*   **کاربران A و B:** آیتم‌های مشترک رتبه‌بندی شده: {b, d, e, g}
    *   بردارهای مشترک: $V_A'' = [1.667, 1.667, -2.333, -0.333]$ و $V_B'' = [0.667, 0.667, -1.333, -1.333]$
    *   حاصل ضرب داخلی $V_A'' \cdot V_B'' = (1.667 \times 0.667) + (1.667 \times 0.667) + (-2.333 \times -1.333) + (-0.333 \times -1.333) \approx 1.1115 + 1.1115 + 3.1099 + 0.4435 = 5.7764$.
    *   اندازه $||V_A''|| = \sqrt{1.667^2+1.667^2+(-2.333)^2+(-0.333)^2} = \sqrt{2.7789+2.7789+5.4429+0.1109} = \sqrt{11.1116} \approx 3.333$.
    *   اندازه $||V_B''|| = \sqrt{0.667^2+0.667^2+(-1.333)^2+(-1.333)^2} = \sqrt{0.4449+0.4449+1.7769+1.7769} = \sqrt{4.4436} \approx 2.108$.
    *   شباهت کسینوسی $sim(A,B) = \frac{5.7764}{3.333 \cdot 2.108} \approx \frac{5.7764}{7.026} \approx 0.8222$.
    *   **فاصله کسینوسی $d(A,B) = 1 - 0.8222 = 0.1778$**.

*   **کاربران A و C:** آیتم‌های مشترک رتبه‌بندی شده: {a, d, g, h}
    *   بردارهای مشترک: $V_A'' = [0.667, 1.667, -0.333, -1.333]$ و $V_C'' = [-1.0, 0.0, 2.0, 0.0]$
    *   حاصل ضرب داخلی $V_A'' \cdot V_C'' = (0.667 \times -1.0) + (1.667 \times 0.0) + (-0.333 \times 2.0) + (-1.333 \times 0.0) \approx -0.667 + 0.0 - 0.666 + 0.0 = -1.333$.
    *   اندازه $||V_A''|| = \sqrt{0.667^2+1.667^2+(-0.333)^2+(-1.333)^2} = \sqrt{0.4449+2.7789+0.1109+1.7769} = \sqrt{5.1116} \approx 2.261$.
    *   اندازه $||V_C''|| = \sqrt{(-1.0)^2+0.0^2+2.0^2+0.0^2} = \sqrt{1.0+0.0+4.0+0.0} = \sqrt{5.0} \approx 2.236$.
    *   شباهت کسینوسی $sim(A,C) = \frac{-1.333}{2.261 \cdot 2.236} \approx \frac{-1.333}{5.056} \approx -0.2636$.
    *   **فاصله کسینوسی $d(A,C) = 1 - (-0.2636) = 1.2636$**.

*   **کاربران B و C:** آیتم‌های مشترک رتبه‌بندی شده: {c, d, f, g}
    *   بردارهای مشترک: $V_B'' = [1.667, 0.667, -0.333, -1.333]$ و $V_C'' = [-2.0, 0.0, 1.0, 2.0]$
    *   حاصل ضرب داخلی $V_B'' \cdot V_C'' = (1.667 \times -2.0) + (0.667 \times 0.0) + (-0.333 \times 1.0) + (-1.333 \times 2.0) \approx -3.334 + 0.0 - 0.333 - 2.666 = -6.333$.
    *   اندازه $||V_B''|| = \sqrt{1.667^2+0.667^2+(-0.333)^2+(-1.333)^2} = \sqrt{2.7789+0.4449+0.1109+1.7769} = \sqrt{5.1116} \approx 2.261$.
    *   اندازه $||V_C''|| = \sqrt{(-2.0)^2+0.0^2+1.0^2+2.0^2} = \sqrt{4.0+0.0+1.0+4.0} = \sqrt{9.0} = 3.0$.
    *   شباهت کسینوسی $sim(B,C) = \frac{-6.333}{2.261 \cdot 3.0} \approx \frac{-6.333}{6.783} \approx -0.9337$.
    *   **فاصله کسینوسی $d(B,C) = 1 - (-0.9337) = 1.9337$**.

---


**ترجمه تمرین 9.3.2:**
در این تمرین، آیتم‌های موجود در ماتریس شکل 9.8 را خوشه‌بندی می‌کنیم. مراحل زیر را انجام دهید:

(الف) هشت آیتم را به صورت سلسله‌مراتبی به چهار خوشه خوشه‌بندی کنید. روش خوشه‌بندی باید به این صورت باشد: همه امتیازات 3، 4 و 5 را با 1 و امتیازات 1، 2 و خانه‌های خالی را با 0 جایگزین کنید. از فاصله جاکارد برای اندازه‌گیری فاصله بین بردارهای ستونی حاصل استفاده کنید. برای خوشه‌هایی با بیش از یک عنصر، فاصله بین خوشه‌ها را حداقل فاصله بین جفت عناصر، یکی از هر خوشه، در نظر بگیرید.

(ب) سپس، از ماتریس اصلی شکل 9.8، یک ماتریس جدید بسازید که ردیف‌های آن مانند قبل مربوط به کاربران و ستون‌های آن مربوط به خوشه‌ها باشند. مقدار هر ورودی برای یک کاربر و یک خوشه از آیتم‌ها را با میانگین‌گیری از ورودی‌های غیرخالی برای آن کاربر و همه آیتم‌های آن خوشه محاسبه کنید.

(ج) فاصله کسینوسی بین هر جفت کاربر را با استفاده از ماتریس حاصل از بخش (ب) محاسبه کنید.

**داده‌های ماتریس شکل 9.8**:
(خالی به معنای امتیازدهی نشده است)
| آیتم | a | b | c | d | e | f | g | h |
|------|---|---|---|---|---|---|---|---|
| کاربر A | 4 | 5 |   | 5 | 1 |   | 3 | 2 |
| کاربر B |   | 3 | 4 | 3 | 1 | 2 | 1 |   |
| کاربر C | 2 |   | 1 | 3 |   | 4 | 5 | 3 |

---

**پاسخ به تمرین:**

**بخش (الف): خوشه‌بندی سلسله‌مراتبی آیتم‌ها**

**مرحله 1: تبدیل ماتریس امتیازات به مقادیر دودویی**
امتیازات 3، 4، و 5 به 1 تبدیل می‌شوند و امتیازات 1، 2، و خانه‌های خالی به 0 تبدیل می‌شوند.

| آیتم | a | b | c | d | e | f | g | h |
|------|---|---|---|---|---|---|---|---|
| کاربر A | 1 | 1 | 0 | 1 | 0 | 0 | 1 | 0 |
| کاربر B | 0 | 1 | 1 | 1 | 0 | 0 | 0 | 0 |
| کاربر C | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 |

اکنون، هر ستون (آیتم) را به عنوان یک مجموعه دودویی در نظر می‌گیریم. به عنوان مثال، مجموعه آیتم 'a' برای کاربران [A, B, C] به صورت {A} (چون A=1 و B=0 و C=0) نمایش داده می‌شود.

*   Ca = {A}
*   Cb = {A, B}
*   Cc = {B}
*   Cd = {A, B, C}
*   Ce = {}
*   Cf = {C}
*   Cg = {A, C}
*   Ch = {C}

**مرحله 2: محاسبه فاصله جاکارد بین تمام جفت آیتم‌ها**
فاصله جاکارد (d) برابر است با 1 منهای شباهت جاکارد (sim).
شباهت جاکارد بین دو مجموعه S1 و S2 به صورت |S1 ∩ S2| / |S1 ∪ S2| تعریف می‌شود.

برخی از فواصل کلیدی (کوچک‌ترین‌ها در ابتدا):
*   d(Cf, Ch) = 1 - |{C}| / |{C}| = 1 - 1/1 = **0**
*   d(Cb, Cd) = 1 - |{A,B}| / |{A,B,C}| = 1 - 2/3 = **0.333**
*   d(Cd, Cg) = 1 - |{A,C}| / |{A,B,C}| = 1 - 2/3 = **0.333**
*   d(Ca, Cb) = 1 - |{A}| / |{A,B}| = 1 - 1/2 = **0.5**
*   d(Ca, Cg) = 1 - |{A}| / |{A,C}| = 1 - 1/2 = **0.5**
*   d(Cb, Cc) = 1 - |{B}| / |{A,B}| = 1 - 1/2 = **0.5**
*   d(Cf, Cg) = 1 - |{C}| / |{A,C}| = 1 - 1/2 = **0.5**
*   d(Cg, Ch) = 1 - |{C}| / |{A,C}| = 1 - 1/2 = **0.5**
*   d(Cc, Ce) = 1 - |{}| / |{B}| = 1 - 0/1 = **1** (و سایر مواردی که اشتراک صفر و اجتماع غیرصفر دارند)

**مرحله 3: خوشه‌بندی سلسله‌مراتبی (Single-linkage)**
ما با 8 خوشه شروع می‌کنیم و در هر مرحله نزدیکترین جفت خوشه‌ها را ادغام می‌کنیم تا به 4 خوشه برسیم. فاصله بین خوشه‌ها (اگر شامل بیش از یک عنصر باشند) برابر با حداقل فاصله جاکارد بین هر جفت عنصر، یکی از هر خوشه، است.

1.  **ادغام {f} و {h}**:
    *   فاصله: d(Cf, Ch) = **0**.
    *   خوشه جدید: **K1 = {f, h}**.
    *   تعداد خوشه‌ها: 7 ({a}, {b}, {c}, {d}, {e}, {g}, K1)

2.  **ادغام {b} و {d}**: (کوچکترین فاصله غیرصفر 0.333)
    *   فاصله: d(Cb, Cd) = **0.333**.
    *   خوشه جدید: **K2 = {b, d}**.
    *   تعداد خوشه‌ها: 6 ({a}, {c}, {e}, {g}, K1, K2)
    *   **بررسی فواصل جدید**:
        *   فاصله {g} به K2 = {b, d}: min(d(Cg,Cb), d(Cg,Cd)) = min(0.667, 0.333) = **0.333**.

3.  **ادغام {g} و K2 = {b, d}**: (کوچکترین فاصله 0.333)
    *   فاصله: d(Cg, K2) = **0.333**.
    *   خوشه جدید: **K3 = {b, d, g}**.
    *   تعداد خوشه‌ها: 5 ({a}, {c}, {e}, K1, K3)
    *   **بررسی فواصل جدید**:
        *   فاصله {a} به K3 = {b, d, g}: min(d(Ca,Cb), d(Ca,Cd), d(Ca,Cg)) = min(0.5, 0.667, 0.5) = **0.5**.
        *   فاصله {c} به K3 = {b, d, g}: min(d(Cc,Cb), d(Cc,Cd), d(Cc,Cg)) = min(0.5, 0.667, 1) = **0.5**.
        *   فاصله K1 = {f, h} به K3 = {b, d, g}: min(d(Cf,Cb), d(Cf,Cd), d(Cf,Cg), d(Ch,Cb), d(Ch,Cd), d(Ch,Cg)) = min(1, 0.667, 0.5, 1, 0.667, 0.5) = **0.5**.

4.  **ادغام {a} و K3 = {b, d, g}**: (چندین جفت با فاصله 0.5 وجود دارد، به صورت دلخواه {a} و K3 انتخاب می‌شود.)
    *   فاصله: d(Ca, K3) = **0.5**.
    *   خوشه جدید: **K4 = {a, b, d, g}**.
    *   تعداد خوشه‌ها: 4 ({c}, {e}, K1, K4)

**نتیجه بخش (الف): چهار خوشه نهایی عبارتند از:**
*   **خوشه 1: {f, h}**
*   **خوشه 2: {b, d, g}**
*   **خوشه 3: {a}**
*   **خوشه 4: {c}**
*   **خوشه 5: {e}**

**تجدیدنظر در خوشه‌بندی بخش (الف):**
به نظر می‌رسد انتخاب دلخواه در مرحله 4 باعث شد که به 5 خوشه برسیم. باید تا زمانی که دقیقاً 4 خوشه داریم، ادغام را ادامه دهیم. با توجه به اینکه در مرحله 4 چندین گزینه با فاصله 0.5 وجود داشت، یک ادغام دیگر باید انجام شود تا به 4 خوشه برسیم.
در مرحله 4، ما 5 خوشه داریم: {a}, {c}, {e}, K1={f,h}, K3={b,d,g}.
کوچکترین فاصله موجود در این مرحله 0.5 است. می‌توانیم یکی از ادغام‌های زیر را انجام دهیم تا به 4 خوشه برسیم:
*   {a} با K3
*   {c} با K3
*   K1 با K3

اگر {a} با K3 ادغام شود، خوشه‌های نهایی: **{a, b, d, g}, {c}, {e}, {f, h}** (این 4 خوشه نهایی هستند).

**نتیجه تصحیح‌شده بخش (الف):**
*   **C1 = {a, b, d, g}**
*   **C2 = {f, h}**
*   **C3 = {c}**
*   **C4 = {e}**

---

**بخش (ب): ساخت ماتریس جدید بر اساس خوشه‌های آیتم‌ها**

ماتریس جدید دارای ردیف‌های کاربران و ستون‌های خوشه‌های آیتم‌ها (C1، C2، C3، C4) خواهد بود. هر ورودی میانگین امتیازات غیرخالی کاربر برای آیتم‌های آن خوشه در ماتریس *اصلی* است.

ماتریس اصلی (شکل 9.8):
| آیتم | a | b | c | d | e | f | g | h |
|------|---|---|---|---|---|---|---|---|
| کاربر A | 4 | 5 |   | 5 | 1 |   | 3 | 2 |
| کاربر B |   | 3 | 4 | 3 | 1 | 2 | 1 |   |
| کاربر C | 2 |   | 1 | 3 |   | 4 | 5 | 3 |

**محاسبات برای ماتریس جدید:**

*   **کاربر A:**
    *   **C1 = {a, b, d, g}**: امتیازات A برای این آیتم‌ها: (a=4, b=5, d=5, g=3). میانگین: (4+5+5+3)/4 = 17/4 = **4.25**
    *   **C2 = {f, h}**: امتیازات A برای این آیتم‌ها: (h=2). میانگین: 2/1 = **2**
    *   **C3 = {c}**: امتیاز A برای c: خالی. ورودی **خالی** باقی می‌ماند.
    *   **C4 = {e}**: امتیاز A برای e: (e=1). میانگین: 1/1 = **1**

*   **کاربر B:**
    *   **C1 = {a, b, d, g}**: امتیازات B برای این آیتم‌ها: (b=3, d=3, g=1). (a خالی است). میانگین: (3+3+1)/3 = 7/3 = **2.33**
    *   **C2 = {f, h}**: امتیازات B برای این آیتم‌ها: (f=2). (h خالی است). میانگین: 2/1 = **2**
    *   **C3 = {c}**: امتیاز B برای c: (c=4). میانگین: 4/1 = **4**
    *   **C4 = {e}**: امتیاز B برای e: (e=1). میانگین: 1/1 = **1**

*   **کاربر C:**
    *   **C1 = {a, b, d, g}**: امتیازات C برای این آیتم‌ها: (a=2, d=3, g=5). (b خالی است). میانگین: (2+3+5)/3 = 10/3 = **3.33**
    *   **C2 = {f, h}**: امتیازات C برای این آیتم‌ها: (f=4, h=3). میانگین: (4+3)/2 = 7/2 = **3.5**
    *   **C3 = {c}**: امتیاز C برای c: (c=1). میانگین: 1/1 = **1**
    *   **C4 = {e}**: امتیاز C برای e: خالی. ورودی **خالی** باقی می‌ماند.

**ماتریس جدید (بخش ب):**
| کاربر | C1={a,b,d,g} | C2={f,h} | C3={c} | C4={e} |
|-------|---------------|----------|--------|--------|
| A     | 4.25          | 2        |        | 1      |
| B     | 2.33          | 2        | 4      | 1      |
| C     | 3.33          | 3.5      | 1      |        |

---

**بخش (ج): محاسبه فاصله کسینوسی بین هر جفت کاربر**

برای محاسبه فاصله کسینوسی، خانه‌های خالی را 0 در نظر می‌گیریم.
*   بردار کاربر A: **VA = [4.25, 2, 0, 1]**
*   بردار کاربر B: **VB = [2.33, 2, 4, 1]**
*   بردار کاربر C: **VC = [3.33, 3.5, 1, 0]**

**فرمول شباهت کسینوسی (sim):** (X . Y) / (||X|| ||Y||)
**فرمول فاصله کسینوسی (d):** 1 - sim(X, Y)

**1. فاصله کسینوسی بین کاربر A و B:**
*   VA . VB = (4.25 * 2.33) + (2 * 2) + (0 * 4) + (1 * 1) = 9.9025 + 4 + 0 + 1 = **14.9025**
*   ||VA|| = sqrt(4.25^2 + 2^2 + 0^2 + 1^2) = sqrt(18.0625 + 4 + 0 + 1) = sqrt(23.0625) = **4.8023**
*   ||VB|| = sqrt(2.33^2 + 2^2 + 4^2 + 1^2) = sqrt(5.4289 + 4 + 16 + 1) = sqrt(26.4289) = **5.1409**
*   sim(A, B) = 14.9025 / (4.8023 * 5.1409) = 14.9025 / 24.6983 = **0.6034**
*   **d(A, B) = 1 - 0.6034 = 0.3966**

**2. فاصله کسینوسی بین کاربر A و C:**
*   VA . VC = (4.25 * 3.33) + (2 * 3.5) + (0 * 1) + (1 * 0) = 14.1525 + 7 + 0 + 0 = **21.1525**
*   ||VA|| = **4.8023** (قبلاً محاسبه شده)
*   ||VC|| = sqrt(3.33^2 + 3.5^2 + 1^2 + 0^2) = sqrt(11.0889 + 12.25 + 1 + 0) = sqrt(24.3389) = **4.9334**
*   sim(A, C) = 21.1525 / (4.8023 * 4.9334) = 21.1525 / 23.6997 = **0.8925**
*   **d(A, C) = 1 - 0.8925 = 0.1075**

**3. فاصله کسینوسی بین کاربر B و C:**
*   VB . VC = (2.33 * 3.33) + (2 * 3.5) + (4 * 1) + (1 * 0) = 7.7589 + 7 + 4 + 0 = **18.7589**
*   ||VB|| = **5.1409** (قبلاً محاسبه شده)
*   ||VC|| = **4.9334** (قبلاً محاسبه شده)
*   sim(B, C) = 18.7589 / (5.1409 * 4.9334) = 18.7589 / 25.3664 = **0.7395**
*   **d(B, C) = 1 - 0.7395 = 0.2605**

**خلاصه فواصل کسینوسی بین کاربران:**
*   **d(A, B) = 0.3966**
*   **d(A, C) = 0.1075**
*   **d(B, C) = 0.2605**
---



در ادامه به ترجمه و حل تمرینات درخواست شده می‌پردازم:

---

### **تمرین‌های بخش 9.4**

**تمرین 9.4.1**
**ترجمه فارسی:**
با شروع از تجزیه ماتریس در شکل 9.10، می‌توانیم هر یک از 20 درایه ماتریس‌های U یا V را برای بهینه‌سازی انتخاب کنیم. این مرحله بهینه‌سازی اولیه را با فرض انتخاب:
(الف) `u32`
(ب) `v41`
انجام دهید.

**توضیح و پاسخ تمرین 9.4.1:**

در این تمرین، هدف بهینه‌سازی یک درایه واحد از ماتریس U یا V با استفاده از روش **نزول گرادیان (Gradient Descent)** است. این روش به دنبال یافتن مقداری برای درایه مورد نظر است که **خطای میانگین مربع ریشه (RMSE)** بین ماتریس اصلی M و حاصل‌ضرب U و V (یعنی UV) را به حداقل برساند. فرمول عمومی برای بهینه‌سازی درایه `u_rs` (در ماتریس U) یا `v_rs` (در ماتریس V) به شرح زیر است:

برای بهینه‌سازی `u_rs` (درایه در سطر r و ستون s ماتریس U):
`x = (∑_j v_sj * (m_rj - ∑_{k≠s} u_rk v_kj)) / (∑_j v_sj^2)`
که در آن:
*   `x` مقدار جدید `u_rs` است که به دنبال آن هستیم.
*   `r` و `s` به ترتیب شماره سطر و ستون در ماتریس U هستند.
*   `j` روی ستون‌هایی از M تکرار می‌شود که درایه `m_rj` (درایه مربوطه در M) خالی نباشد.
*   `m_rj` درایه مربوطه در ماتریس اصلی M است.
*   `v_sj` درایه‌های سطر `s` از ماتریس V هستند.
*   `u_rk` درایه‌های سطر `r` از ماتریس U (به جز `u_rs`) هستند.
*   `v_kj` درایه‌های ستون `j` از ماتریس V (به جز `v_sj`) هستند.
*   `∑_{k≠s}` به معنی جمع روی تمام مقادیر `k` بین 1 و `d` (تعداد مفهوم‌ها یا ابعاد کاهش‌یافته) به جز `k=s` است. (در این مثال `d=2` است زیرا U ماتریس 5x2 و V ماتریس 2x5 است).

**ماتریس‌های اولیه:**
از شکل 9.10، ماتریس‌های اولیه U و V که تمام درایه‌های آن‌ها 1 هستند، به شرح زیر است:
**U** =
```
[ 1 1 ]
[ 1 1 ]
[ 1 1 ]
[ 1 1 ]
[ 1 1 ]
```
**V** =
```
[ 1 1 1 1 1 ]
[ 1 1 1 1 1 ]
```
و ماتریس اصلی **M** از شکل 9.9 به شرح زیر است:
**M** =
```
[ 5  2  4  4  3 ]
[ 3  3  1  2  4 ]
[ 2 BLANK 1  4  2 ]
[ 2  5  4  1  5 ]
[ 4  4  5 BLANK 4 ]
```
(درایه‌های BLANK به معنی نامعلوم هستند و در محاسبات مربوط به RMSE نادیده گرفته می‌شوند).

**الف) بهینه‌سازی `u32`:**
درایه `u32` به معنی درایه در **سطر 3، ستون 2** ماتریس U است. بنابراین `r=3` و `s=2`.
فرمول بهینه‌سازی `u_rs` می‌شود:
`x = (∑_j v_2j * (m_3j - u_31 * v_1j)) / (∑_j v_2j^2)`
مقادیر درگیر:
*   `u_31` (درایه سطر 3، ستون 1 از U اولیه) = **1**.
*   `v_1j` (سطر 1 از V اولیه) = ``.
*   `v_2j` (سطر 2 از V اولیه) = ``.
*   `m_3j` (سطر 3 از M): `[2, BLANK, 1, 4, 2]`.
درایه‌های غیرخالی در `m_3j` برای `j = 1, 3, 4, 5` هستند.

**محاسبه مخرج کسر:**
`∑_j v_2j^2` بر روی ستون‌های غیرخالی M در سطر 3: `v_21^2 + v_23^2 + v_24^2 + v_25^2`
`= 1^2 + 1^2 + 1^2 + 1^2 = 1 + 1 + 1 + 1 = 4`

**محاسبه صورت کسر:**
`∑_j v_2j * (m_3j - u_31 * v_1j)`
`j=1`: `v_21 * (m_31 - u_31 * v_11) = 1 * (2 - 1 * 1) = 1 * (1) = 1`
`j=2`: `m_32` خالی است، بنابراین این ترم نادیده گرفته می‌شود.
`j=3`: `v_23 * (m_33 - u_31 * v_13) = 1 * (1 - 1 * 1) = 1 * (0) = 0`
`j=4`: `v_24 * (m_34 - u_31 * v_14) = 1 * (4 - 1 * 1) = 1 * (3) = 3`
`j=5`: `v_25 * (m_35 - u_31 * v_15) = 1 * (2 - 1 * 1) = 1 * (1) = 1`
**جمع صورت کسر:** `1 + 0 + 3 + 1 = 5`

**مقدار بهینه‌شده `u32`:** `x = 5 / 4 = 1.25`
**پاسخ:** مقدار بهینه‌شده `u32` برابر با **1.25** است.

---

**ب) بهینه‌سازی `v41`:**
**نکته مهم در مورد `v41`:** ماتریس V در مثال‌های کتاب (شکل 9.9 و 9.10) یک ماتریس 2×5 است، یعنی دارای 2 سطر و 5 ستون. در نتیجه درایه‌ای به نام `v41` (در سطر 4، ستون 1) در آن وجود ندارد. این یک ابهام در صورت سوال است. با توجه به متن تمرین که می‌گوید "هر یک از 20 درایه ماتریس‌های U یا V" (که 5x2=10 برای U و 2x5=10 برای V است و مجموعا 20 درایه می‌شود)، فرض می‌کنیم سوال به اشتباه نوشته شده و منظور **`v14`** (درایه سطر 1، ستون 4 از V) بوده است، زیرا `v14` یک درایه معتبر در ماتریس 2x5 V است.

با فرض اینکه منظور سوال `v14` (درایه در سطر 1، ستون 4 ماتریس V) است، بنابراین `r=1` (اشاره به ستون 1 از U و سطر 1 از V، یعنی مفهوم اول) و `s=4` (اشاره به ستون 4 از M و V، یعنی آیتم چهارم).

فرمول بهینه‌سازی `v_rs` (درایه در سطر r و ستون s ماتریس V):
`y = (∑_i u_ir * (m_is - ∑_{k≠r} u_ik v_ks)) / (∑_i u_ir^2)`
که در آن:
*   `y` مقدار جدید `v_rs` است که به دنبال آن هستیم.
*   `r` و `s` به ترتیب شماره سطر و ستون در ماتریس V (در اینجا `r=1, s=4`).
*   `i` روی سطر‌هایی از M تکرار می‌شود که درایه `m_is` (درایه مربوطه در M) خالی نباشد.
*   `u_ir` درایه‌های ستون `r` از ماتریس U هستند.
*   `m_is` درایه مربوطه در ماتریس اصلی M است.
*   `u_ik` درایه‌های سطر `i` از ماتریس U (به جز `u_ir`) هستند.
*   `v_ks` درایه‌های ستون `s` از ماتریس V (به جز `v_rs`) هستند.
*   `∑_{k≠r}` به معنی جمع روی تمام مقادیر `k` بین 1 و `d` (تعداد مفهوم‌ها، که اینجا `d=2` است) به جز `k=r` است.

مقادیر درگیر:
*   `u_i1` (ستون 1 از U اولیه) = ``.
*   `u_i2` (ستون 2 از U اولیه) = ``.
*   `v_24` (درایه سطر 2، ستون 4 از V اولیه) = **1**.
*   `m_i4` (ستون 4 از M): ``. (تمامی درایه‌ها غیرخالی هستند)
درایه‌های غیرخالی در `m_i4` برای `i = 1, 2, 3, 4, 5` هستند.

**محاسبه مخرج کسر:**
`∑_i u_i1^2` بر روی تمام سطرها (چون تمام درایه‌های `m_i4` غیرخالی هستند):
`= 1^2 + 1^2 + 1^2 + 1^2 + 1^2 = 1 + 1 + 1 + 1 + 1 = 5`

**محاسبه صورت کسر:**
`∑_i u_i1 * (m_i4 - u_i2 * v_24)`
`i=1`: `u_11 * (m_14 - u_12 * v_24) = 1 * (4 - 1 * 1) = 1 * (3) = 3`
`i=2`: `u_21 * (m_24 - u_22 * v_24) = 1 * (2 - 1 * 1) = 1 * (1) = 1`
`i=3`: `u_31 * (m_34 - u_32 * v_24) = 1 * (4 - 1 * 1) = 1 * (3) = 3`
`i=4`: `u_41 * (m_44 - u_42 * v_24) = 1 * (1 - 1 * 1) = 1 * (0) = 0`
`i=5`: `u_51 * (m_54 - u_52 * v_24) = 1 * (5 - 1 * 1) = 1 * (4) = 4`
**جمع صورت کسر:** `3 + 1 + 3 + 0 + 4 = 11`

**مقدار بهینه‌شده `v14` (با فرض `v41` typo):** `y = 11 / 5 = 2.2`
**پاسخ:** با فرض اینکه منظور سوال `v14` بوده است، مقدار بهینه‌شده آن برابر با **2.2** است.

---

**تمرین 9.4.2**
**ترجمه فارسی:**
اگر بخواهیم مانند شکل 9.10، تمام درایه‌های U و V را با یک مقدار ثابت یکسان مقداردهی اولیه کنیم، چه مقداری **خطای میانگین مربع ریشه (RMSE)** را برای ماتریس M مثال مورد بررسی (Running Example) به حداقل می‌رساند؟

**توضیح و پاسخ تمرین 9.4.2:**

در این تمرین، فرض می‌کنیم تمام درایه‌های ماتریس U و V برابر با یک مقدار ثابت `c` هستند.
ماتریس U ابعاد `n × d` دارد (در اینجا 5×2) و ماتریس V ابعاد `d × m` دارد (در اینجا 2×5). بنابراین `d=2`.
اگر تمام درایه‌های U و V برابر با `c` باشند، آنگاه هر درایه `p_ij` در ماتریس حاصل‌ضرب `P = UV` برابر خواهد بود با:
`p_ij = ∑_{k=1 to d} (u_ik * v_kj)`
`p_ij = ∑_{k=1 to 2} (c * c) = c^2 + c^2 = 2c^2`

پس، ماتریس `P` یک ماتریس 5×5 است که تمام درایه‌های آن `2c^2` هستند.
هدف ما حداقل کردن **مجموع مربعات خطاها (Sum of Squared Errors)** است. این مقدار برابر است با:
`E = ∑_i ∑_j (m_ij - p_ij)^2`
که در آن `i` و `j` روی تمام درایه‌های غیرخالی `m_ij` در ماتریس M تکرار می‌شوند.
`E = ∑_i ∑_j (m_ij - 2c^2)^2`

برای یافتن `c` که `E` را حداقل می‌کند، باید مشتق `E` نسبت به `c` را محاسبه کرده و آن را برابر با صفر قرار دهیم:
`dE/dc = ∑_i ∑_j 2 * (m_ij - 2c^2) * (-4c) = 0`
با تقسیم بر `(-8c)` (با فرض `c ≠ 0`):
`∑_i ∑_j (m_ij - 2c^2) = 0`
`∑_i ∑_j m_ij - ∑_i ∑_j 2c^2 = 0`

**جمع تمام درایه‌های غیرخالی ماتریس M:**
*   سطر 1: `5 + 2 + 4 + 4 + 3 = 18`
*   سطر 2: `3 + 3 + 1 + 2 + 4 = 13`
*   سطر 3: `2 + 1 + 4 + 2 = 9` (درایه خالی نادیده گرفته شد)
*   سطر 4: `2 + 5 + 4 + 1 + 5 = 17`
*   سطر 5: `4 + 4 + 5 + 4 = 17` (درایه خالی نادیده گرفته شد)
**مجموع کل درایه‌های غیرخالی M:** `18 + 13 + 9 + 17 + 17 = 74`

**تعداد درایه‌های غیرخالی M:** ماتریس M دارای 25 درایه است که 2 درایه آن خالی هستند. بنابراین تعداد درایه‌های غیرخالی `25 - 2 = 23` است.

اکنون معادله را حل می‌کنیم:
`74 - (23 * 2c^2) = 0`
`74 - 46c^2 = 0`
`46c^2 = 74`
`c^2 = 74 / 46 = 37 / 23`
`c = √(37 / 23)`

`c ≈ √1.60869565 ≈ 1.26834`

**پاسخ:** مقداری که تمام درایه‌های U و V باید به آن مقداردهی اولیه شوند تا RMSE به حداقل برسد، تقریباً برابر با **1.268** است.

---

متن زیر به فرمت مارکدان برای نمایش در گیتهاب آماده شده است:

# راه حل تمرین‌های کتاب A Programmer's Guide to Data Mining

-----

## تمرین ۹.۴.۱

**ترجمه:**
"با شروع از تجزیه ماتریس در شکل ۹.۱۰، می‌توانیم هر یک از ۲۰ ورودی در U یا V را برای بهینه‌سازی انتخاب کنیم. این مرحله بهینه‌سازی اولیه را با فرض انتخاب:
(الف) $u\_{32}$
(ب) $v\_{41}$
انجام دهید."

### پاسخ:

**نکته مهم:** قبل از شروع حل، لازم به ذکر است که در بخش (ب) این تمرین، $v\_{41}$ به عنوان ورودی برای بهینه‌سازی خواسته شده است. با توجه به اینکه ماتریس V (شکل ۹.۹ و ۹.۱۰) دارای ابعاد ۲x۵ است، ورودی $v\_{41}$ (سطر ۴، ستون ۱) خارج از محدوده این ماتریس قرار می‌گیرد. این یک ابهام در صورت سوال است. بنابراین، پاسخ تنها به بخش (الف) تمرین داده می‌شود و برای بخش (ب)، نیاز به شفاف‌سازی از سوی شما وجود دارد.

### حل بخش (الف): بهینه‌سازی $u\_{32}$

هدف ما بهینه‌سازی ورودی $u\_{32}$ (عنصر سطر ۳ و ستون ۲ ماتریس U) است. مقدار اولیه این عنصر از شکل ۹.۱۰ برابر با **۱** است.

#### مراحل حل:

۱. **شناسایی مقادیر اولیه:**

  - ماتریس $M$ از شکل ۹.۹:
    ```
    M = [
        [5, 2, BLANK, 4, 3],
        [3, 3, 1, 2, 4],
        [1, 2, BLANK, 3, 1],
        [4, 2, 5, 4, 3],
        [5, 4, 4, 5, BLANK]
    ]
    ```
  - ماتریس‌های اولیه $U$ و $V$ از شکل ۹.۱۰ (همه ورودی‌ها ۱ هستند):
    ```
    U = [
        [1, 1],
        [1, 1],
        [1, 1],  <-- Row 3
        [1, 1],
        [1, 1]
    ]
    ```
    ```
    V = [
        [1, 1, 1, 1, 1],  <-- Row 1
        [1, 1, 1, 1, 1]   <-- Row 2
    ]
    ```
  - شاخص‌های مورد نظر برای $u\_{32}$ عبارتند از: $r = 3$ (سطر) و $s = 2$ (ستون).
  - بعد مشترک $d = 2$ است.

۲. **فرمول بهینه‌سازی:**
برای بهینه‌سازی عنصر $u\_{rs}$ (که در اینجا $u\_{32}$ است)، از فرمول زیر استفاده می‌کنیم:
$x = \\frac{\\sum\_{j} v\_{sj} \\cdot (m\_{rj} - \\sum\_{k\\neq s} u\_{rk} \\cdot v\_{kj})}{\\sum\_{j} v\_{sj}^2}$

  - $x$ مقدار بهینه جدید برای $u\_{rs}$ است.
  - $\\sum\_{j}$ به معنای جمع روی تمام ستون‌های $j$ است که $m\_{rj}$ (عنصر متناظر در ماتریس $M$) خالی (**BLANK**) نیستند.
  - $k\\neq s$ به معنای جمع روی تمام $k$ها از ۱ تا $d$ است، به جز $k = s$.

۳. **محاسبه مخرج کسر:**
مخرج کسر عبارت است از: $\\sum\_{j} v\_{sj}^2$

  - در اینجا $s = 2$ است، بنابراین ما به مقادیر سطر دوم ماتریس $V$ نیاز داریم: $v\_{21}, v\_{22}, v\_{23}, v\_{24}, v\_{25}$.
  - با توجه به ماتریس اولیه $V$، تمام این مقادیر **۱** هستند.
  - عناصر غیرخالی سطر $r = 3$ ماتریس $M$ عبارتند از: $m\_{31}, m\_{32}, m\_{34}, m\_{35}$ ($m\_{33}$ خالی است).
  - بنابراین، $j$ها که در جمع شرکت می‌کنند عبارتند از ۱, ۲, ۴, ۵.
  - مخرج = $v\_{21}^2 + v\_{22}^2 + v\_{24}^2 + v\_{25}^2 = 1^2 + 1^2 + 1^2 + 1^2 = 1 + 1 + 1 + 1 = 4$

۴. **محاسبه صورت کسر:**
صورت کسر عبارت است از: $\\sum\_{j} v\_{sj} \\cdot (m\_{rj} - \\sum\_{k\\neq s} u\_{rk} \\cdot v\_{kj})$

  - در اینجا $r = 3$ و $s = 2$.
  - $k\\neq s$ به معنای $k\\neq 2$ است، بنابراین $k = 1$ (زیرا $d = 2$).
  - عبارت $\\sum\_{k\\neq s} u\_{rk} \\cdot v\_{kj}$ ساده می‌شود به $u\_{r1} \\cdot v\_{1j}$ یعنی $u\_{31} \\cdot v\_{1j}$.
  - از ماتریس اولیه $U$، $u\_{31} = 1$.
  - از ماتریس اولیه $V$، تمام $v\_{1j}$ها ۱ هستند.
  - بنابراین، $u\_{31} \\cdot v\_{1j} = 1 \\cdot 1 = 1$ برای تمام $j$ها.
  - حالا عبارت $m\_{rj} - (u\_{31} \\cdot v\_{1j})$ را برای هر $j$ غیرخالی در سطر ۳ ماتریس $M$ محاسبه می‌کنیم:
      - برای $j = 1$ ($m\_{31} = 1$): $(m\_{31} - u\_{31} \\cdot v\_{11}) = (1 - 1 \\cdot 1) = 0$
      - برای $j = 2$ ($m\_{32} = 2$): $(m\_{32} - u\_{31} \\cdot v\_{12}) = (2 - 1 \\cdot 1) = 1$
      - برای $j = 4$ ($m\_{34} = 3$): $(m\_{34} - u\_{31} \\cdot v\_{14}) = (3 - 1 \\cdot 1) = 2$
      - برای $j = 5$ ($m\_{35} = 1$): $(m\_{35} - u\_{31} \\cdot v\_{15}) = (1 - 1 \\cdot 1) = 0$
  - سپس، این نتایج را در $v\_{sj}$ (یعنی $v\_{2j}$ که همه ۱ هستند) ضرب کرده و با هم جمع می‌کنیم:
      - صورت = $(v\_{21} \\cdot 0) + (v\_{22} \\cdot 1) + (v\_{24} \\cdot 2) + (v\_{25} \\cdot 0)$
      - صورت = $(1 \\cdot 0) + (1 \\cdot 1) + (1 \\cdot 2) + (1 \\cdot 0) = 0 + 1 + 2 + 0 = 3$

۵. **محاسبه مقدار بهینه $x$:**
$x = \\text{صورت} / \\text{مخرج} = 3 / 4 = 0.75$

#### نتیجه:

مقدار بهینه جدید برای $u\_{32}$ برابر با **۰.۷۵** است. پس از این مرحله، ماتریس $U$ به صورت زیر خواهد بود:

```
U = [
    [1, 1],
    [1, 1],
    [1, 0.75],  <-- Updated u32
    [1, 1],
    [1, 1]
]
```

-----

## تمرین ۹.۴.۲

**ترجمه:**
"اگر بخواهیم مانند شکل ۹.۱۰، همه ورودی‌های U و V را با یک مقدار ثابت شروع کنیم، چه مقداری RMSE را برای ماتریس M مثال ما به حداقل می‌رساند؟"

### پاسخ:

هدف این تمرین یافتن یک مقدار ثابت برای تمام ورودی‌های ماتریس‌های **U** و **V** است که کمترین **RMSE** (خطای مجذور میانگین ریشه) را در بازسازی ماتریس **M** ایجاد کند.

#### مراحل حل:

۱. **تعریف مقادیر اولیه و محصول UV:**

  - فرض می‌کنیم که همه ورودی‌های ماتریس $U$ و $V$ دارای یک مقدار ثابت **z** هستند.
  - $U$ یک ماتریس ۵x۲ است که همه $u\_{ik} = z$.
  - $V$ یک ماتریس ۲x۵ است که همه $v\_{kj} = z$.
  - محصول $P = UV$ یک ماتریس ۵x۵ خواهد بود. عنصر $P\_{ij}$ از این محصول به صورت زیر محاسبه می‌شود:
    $P\_{ij} = \\sum\_{k} (u\_{ik} \\cdot v\_{kj})$
  - با توجه به اینکه بعد مشترک $d = 2$ است، جمع روی $k=1, 2$ انجام می‌شود.
    $P\_{ij} = (u\_{i1} \\cdot v\_{1j}) + (u\_{i2} \\cdot v\_{2j}) = (z \\cdot z) + (z \\cdot z) = z^2 + z^2 = 2z^2$
  - بنابراین، تمام عناصر ماتریس $P$ برابر با **$2z^2$** خواهند بود.

۲. **فرمول RMSE:**

  - برای به حداقل رساندن RMSE، باید مجموع مربعات خطاهای بین عناصر ماتریس $M$ و $P$ (که $UV$ است) را برای عناصر غیرخالی $M$ به حداقل برسانیم. این مجموع به صورت زیر است:
    $Sum\_Squared\_Error = \\sum\_{i} \\sum\_{j} (m\_{ij} - P\_{ij})^2$ (برای تمام $m\_{ij}$های غیرخالی)
    $Sum\_Squared\_Error = \\sum\_{i} \\sum\_{j} (m\_{ij} - 2z^2)^2$ (برای تمام $m\_{ij}$های غیرخالی)

۳. **یافتن مقدار ثابت بهینه:**

  - این مشکل معادل یافتن یک مقدار ثابت **C** است که مجموع مربعات تفاوت آن با مجموعه داده‌ها را به حداقل می‌رساند. در آمار، این مقدار **میانگین (Average)** مجموعه داده‌ها است.
  - بنابراین، **$2z^2$** باید برابر با میانگین تمام عناصر غیرخالی ماتریس $M$ باشد.

۴. **محاسبه میانگین عناصر غیرخالی ماتریس M:**

  - لیست عناصر غیرخالی ماتریس $M$ از شکل ۹.۹:
      - سطر ۱: ۵, ۲, ۴, ۴, ۳ (مجموع = ۱۸)
      - سطر ۲: ۳, ۳, ۱, ۲, ۴ (مجموع = ۱۳)
      - سطر ۳: ۱, ۲, ۳, ۱ (مجموع = ۷)
      - سطر ۴: ۴, ۲, ۵, ۴, ۳ (مجموع = ۱۸)
      - سطر ۵: ۵, ۴, ۴, ۵ (مجموع = ۱۸)
  - تعداد کل عناصر غیرخالی = ۵ + ۵ + ۴ + ۵ + ۴ = **۲۳**
  - مجموع کل عناصر غیرخالی = ۱۸ + ۱۳ + ۷ + ۱۸ + ۱۸ = **۷۴**
  - میانگین = $74 / 23 \\approx 3.21739$

۵. **حل برای z:**

  - حالا $2z^2$ را برابر با میانگین محاسبه شده قرار می‌دهیم:
    $2z^2 = 74 / 23$
    $z^2 = 37 / 23$
    $z = \\sqrt{37 / 23}$
    $z \\approx \\sqrt{1.60869565}$
    $z \\approx 1.26834$

#### نتیجه:

مقدار **$z \\approx 1.26834$** برای همه ورودی‌های $U$ و $V$، **RMSE** را برای ماتریس **M** به حداقل می‌رساند.



در ادامه به ترجمه و پاسخ تمرینات بخش 9.4 از منبع "bookL.pdf" پرداخته شده است. برای این حل‌ها، از ماتریس‌های **M**، **U** و **V** ارائه شده در شکل 9.9 و شکل 9.16 استفاده خواهیم کرد. همچنین، فرمول‌های بهینه‌سازی از بخش 9.4.4 مورد استفاده قرار می‌گیرند.

---

**تمرین 9.4.3**
**ترجمه:**
"با شروع از ماتریس‌های **U** و **V** در شکل 9.16، موارد زیر را به ترتیب انجام دهید:
(الف) **u11** را مجدداً بررسی کنید. با توجه به تغییرات اعمال شده تا کنون، بهترین مقدار جدید آن را پیدا کنید.
(ب) سپس بهترین مقدار برای **u52** را انتخاب کنید.
(ج) سپس بهترین مقدار برای **v22** را انتخاب کنید."

**پاسخ:**

**نکته مهم:** در این تمرین، از مقادیر به‌روزشده ماتریس‌های **U** و **V** از شکل 9.16 به عنوان نقطه شروع استفاده می‌شود. این ماتریس‌ها نتیجه بهینه‌سازی‌های قبلی هستند که در مثال‌های منبع انجام شده‌اند.
**ماتریس M از شکل 9.9:**
```
M = [,
    ,
     [1, 2, BLANK, 3, 1],
    ,
     [5, 4, 4, 5, BLANK]]
```
**ماتریس‌های U و V از شکل 9.16:**
```
U = [[2.6,   1],
    ,
     [1.178, 1],
    ,
    ]

V = [[1.617, 1, 1, 1, 1],
    ]
```
ابعاد مشترک **d = 2** است.

**فرمول کلی بهینه‌سازی یک عنصر u_rs:**
`x = (∑_j v_sj * (m_rj - ∑_(k≠s) u_rk * v_kj)) / (∑_j v_sj^2)`
که در آن:
*   **x** مقدار بهینه جدید برای `u_rs` است.
*   **∑_j** روی تمام ستون‌های **j** که `m_rj` (عنصر متناظر در ماتریس M) **خالی (BLANK) نیستند**، جمع می‌زند.
*   **k≠s** به معنای جمع روی تمام `k`ها از 1 تا `d` است، به جز `k = s`.

**فرمول کلی بهینه‌سازی یک عنصر v_rs:**
`y = (∑_i u_ir * (m_is - ∑_(k≠r) u_ik * v_ks)) / (∑_i u_ir^2)`
که در آن:
*   **y** مقدار بهینه جدید برای `v_rs` است.
*   **∑_i** روی تمام سطرهای **i** که `m_is` (عنصر متناظر در ماتریس M) **خالی (BLANK) نیستند**، جمع می‌زند.
*   **k≠r** به معنای جمع روی تمام `k`ها از 1 تا `d` است، به جز `k = r`.

---

**(الف) بهینه‌سازی u11:**
مقدار فعلی `u11` برابر با 2.6 است. (سطر `r=1`، ستون `s=1`)
در این حالت، `k≠s` به معنای `k≠1` است، پس `k=2`.
صورت کسر: `∑_j v_1j * (m_1j - u_12 * v_2j)`
مخرج کسر: `∑_j v_1j^2`
مقادیر **M** در سطر 1 (غیرخالی): `m11=5, m12=2, m13=4, m14=4, m15=3`.
مقادیر فعلی **U** و **V**: `u12 = 1`، `v_1j = [1.617, 1, 1, 1, 1]`، `v_2j =`.

**محاسبه مخرج:** (جمع روی `j = 1, 2, 3, 4, 5` چون همه `m1j` غیرخالی هستند)
`v_11^2 + v_12^2 + v_13^2 + v_14^2 + v_15^2 = (1.617)^2 + 1^2 + 1^2 + 1^2 + 1^2 = 2.6147 + 1 + 1 + 1 + 1 = 6.6147`

**محاسبه صورت:**
*   `j=1 (m11=5)`: `v11 * (m11 - u12*v21) = 1.617 * (5 - 1*1) = 1.617 * 4 = 6.468`
*   `j=2 (m12=2)`: `v12 * (m12 - u12*v22) = 1 * (2 - 1*1) = 1 * 1 = 1`
*   `j=3 (m13=4)`: `v13 * (m13 - u12*v23) = 1 * (4 - 1*1) = 1 * 3 = 3`
*   `j=4 (m14=4)`: `v14 * (m14 - u12*v24) = 1 * (4 - 1*1) = 1 * 3 = 3`
*   `j=5 (m15=3)`: `v15 * (m15 - u12*v25) = 1 * (3 - 1*1) = 1 * 2 = 2`
*   **صورت = 6.468 + 1 + 3 + 3 + 2 = 15.468**

**مقدار بهینه u11:** `15.468 / 6.6147 ≈ 2.338`

**پس از این مرحله، ماتریس U به‌روز می‌شود:**
```
U = [[2.338, 1],
    ,
     [1.178, 1],
    ,
    ]
```
---

**(ب) بهینه‌سازی u52:**
مقدار فعلی `u52` برابر با 1 است. (سطر `r=5`، ستون `s=2`)
در این حالت، `k≠s` به معنای `k≠2` است، پس `k=1`.
صورت کسر: `∑_j v_2j * (m_5j - u_51 * v_1j)`
مخرج کسر: `∑_j v_2j^2`
مقادیر **M** در سطر 5 (غیرخالی): `m51=5, m52=4, m53=4, m54=5` (`m55` خالی است).
مقادیر فعلی **U** و **V** (با u11 جدید): `u51 = 1`، `v_1j = [1.617, 1, 1, 1, 1]`، `v_2j =`.

**محاسبه مخرج:** (جمع روی `j = 1, 2, 3, 4`)
`v_21^2 + v_22^2 + v_23^2 + v_24^2 = 1^2 + 1^2 + 1^2 + 1^2 = 4`

**محاسبه صورت:**
*   `j=1 (m51=5)`: `v21 * (m51 - u51*v11) = 1 * (5 - 1*1.617) = 1 * 3.383 = 3.383`
*   `j=2 (m52=4)`: `v22 * (m52 - u51*v12) = 1 * (4 - 1*1) = 1 * 3 = 3`
*   `j=3 (m53=4)`: `v23 * (m53 - u51*v13) = 1 * (4 - 1*1) = 1 * 3 = 3`
*   `j=4 (m54=5)`: `v24 * (m54 - u51*v14) = 1 * (5 - 1*1) = 1 * 4 = 4`
*   **صورت = 3.383 + 3 + 3 + 4 = 13.383**

**مقدار بهینه u52:** `13.383 / 4 ≈ 3.346`

**پس از این مرحله، ماتریس U دوباره به‌روز می‌شود:**
```
U = [[2.338, 1    ],
    ,
     [1.178, 1    ],
    ,
     [1,     3.346]]  <-- u52 updated
```
---

**(ج) بهینه‌سازی v22:**
مقدار فعلی `v22` برابر با 1 است. (سطر `r=2`، ستون `s=2`)
در این حالت، `k≠r` به معنای `k≠2` است، پس `k=1`.
صورت کسر: `∑_i u_i2 * (m_i2 - u_i1 * v_12)`
مخرج کسر: `∑_i u_i2^2`
مقادیر **M** در ستون 2 (غیرخالی): `m12=2, m22=3, m32=2, m42=2, m52=4`.
**نکته:** در این مرحله از مقادیر **U** به‌روز شده در بخش (ب) استفاده می‌کنیم.
مقادیر فعلی **U** و **V**: `v12 = 1`.
`u_i1 = [2.338, 1, 1.178, 1, 1]`
`u_i2 = [1, 1, 1, 1, 3.346]`

**محاسبه مخرج:** (جمع روی `i = 1, 2, 3, 4, 5`)
`u_12^2 + u_22^2 + u_32^2 + u_42^2 + u_52^2 = 1^2 + 1^2 + 1^2 + 1^2 + (3.346)^2 = 1 + 1 + 1 + 1 + 11.1967 = 15.1967`

**محاسبه صورت:**
*   `i=1 (m12=2)`: `u12 * (m12 - u11*v12) = 1 * (2 - 2.338*1) = 1 * (-0.338) = -0.338`
*   `i=2 (m22=3)`: `u22 * (m22 - u21*v12) = 1 * (3 - 1*1) = 1 * 2 = 2`
*   `i=3 (m32=2)`: `u32 * (m32 - u31*v12) = 1 * (2 - 1.178*1) = 1 * 0.822 = 0.822`
*   `i=4 (m42=2)`: `u42 * (m42 - u41*v12) = 1 * (2 - 1*1) = 1 * 1 = 1`
*   `i=5 (m52=4)`: `u52 * (m52 - u51*v12) = 3.346 * (4 - 1*1) = 3.346 * 3 = 10.038`
*   **صورت = -0.338 + 2 + 0.822 + 1 + 10.038 = 13.522**

**مقدار بهینه v22:** `13.522 / 15.1967 ≈ 0.889`

**پس از این مرحله، ماتریس V به‌روز می‌شود:**
```
V = [[1.617, 1,     1, 1, 1],
     [1,     0.889, 1, 1, 1]]  <-- v22 updated
```

---

**تمرین 9.4.4**
**ترجمه:**
"فرمول y (مقدار بهینه عنصر `v_rs`) را که در پایان بخش 9.4.4 آورده شده است، استخراج کنید."

**پاسخ:**

برای استخراج فرمول بهینه عنصر `v_rs` (که در اینجا آن را `y` نامگذاری می‌کنیم)، رویکرد مشابهی با بهینه‌سازی عنصر `u_rs` دنبال می‌کنیم. هدف ما حداقل کردن خطای مجذور میانگین ریشه (RMSE) بین ماتریس `M` و حاصل‌ضرب `UV` است. بهینه‌سازی یک عنصر خاص `v_rs` تنها بر عناصر ستون `s` از ماتریس حاصل‌ضرب `P = UV` تأثیر می‌گذارد.

1.  **تعریف عنصر `P_is` از ماتریس حاصل‌ضرب `P = UV`:**
    عنصر `P_is` در سطر `i` و ستون `s` ماتریس `P` به صورت زیر تعریف می‌شود:
    `P_is = ∑_k (u_ik * v_ks)`
    در اینجا، `k` از 1 تا `d` (بعد مشترک) متغیر است.

2.  **جدا کردن `v_rs` (که `y` است):**
    برای بهینه‌سازی `v_rs`، آن را به عنوان متغیر `y` در نظر می‌گیریم و از مجموع جدا می‌کنیم. بنابراین، فرمول `P_is` برای هر سطر `i` در ستون `s` به شکل زیر در می‌آید:
    `P_is = ∑_(k≠r) (u_ik * v_ks) + u_ir * y`
    که در آن `∑_(k≠r)` به معنای جمع روی تمام مقادیر `k` از 1 تا `d`، به جز `k = r` است.

3.  **بیان مجموع مربعات خطا برای ستون `s`:**
    ما می‌خواهیم مجموع مربعات خطاهای مربوط به عناصر غیرخالی در ستون `s` ماتریس `M` را حداقل کنیم. این مجموع به صورت زیر است:
    `Sum_Squared_Error = ∑_i (m_is - P_is)^2`
    که در آن `∑_i` به معنای جمع روی تمام سطرهای `i` است که `m_is` (عنصر متناظر در ماتریس `M`) **خالی نیست**.
    با جایگزینی `P_is`، داریم:
    `Sum_Squared_Error = ∑_i (m_is - (∑_(k≠r) u_ik * v_ks + u_ir * y))^2`

4.  **تعریف یک ثابت میانی:**
    برای سادگی، اجازه دهید `C_is` را به این صورت تعریف کنیم:
    `C_is = m_is - ∑_(k≠r) u_ik * v_ks`
    حالا، عبارت `Sum_Squared_Error` به صورت زیر ساده می‌شود:
    `Sum_Squared_Error = ∑_i (C_is - u_ir * y)^2`

5.  **گرفتن مشتق و برابر صفر قرار دادن آن:**
    برای یافتن مقدار `y` که `Sum_Squared_Error` را حداقل می‌کند، از `Sum_Squared_Error` نسبت به `y` مشتق می‌گیریم و آن را برابر با صفر قرار می‌دهیم:
    `d/dy [ ∑_i (C_is - u_ir * y)^2 ] = 0`
    `∑_i [ 2 * (C_is - u_ir * y) * (-u_ir) ] = 0`
    `∑_i [ -2 * u_ir * (C_is - u_ir * y) ] = 0`

6.  **ساده‌سازی و حل برای `y`:**
    ضریب `-2` را می‌توانیم از هر دو طرف حذف کنیم:
    `∑_i [ u_ir * (C_is - u_ir * y) ] = 0`
    `∑_i (u_ir * C_is - u_ir^2 * y) = 0`
    `∑_i (u_ir * C_is) - ∑_i (u_ir^2 * y) = 0`
    `∑_i (u_ir * C_is) = y * ∑_i (u_ir^2)`
    در نهایت، `y` را استخراج می‌کنیم:
    `y = (∑_i u_ir * C_is) / (∑_i u_ir^2)`

7.  **جایگزینی `C_is` به فرم اصلی:**
    با جایگزینی `C_is` به تعریف اولیه‌اش، فرمول نهایی برای مقدار بهینه `y = v_rs` به دست می‌آید:
    **`y = (∑_i u_ir * (m_is - ∑_(k≠r) u_ik * v_ks)) / (∑_i u_ir^2)`**
    این فرمول دقیقاً همان فرمولی است که در پایان بخش 9.4.4 از منبع آورده شده است.

---
